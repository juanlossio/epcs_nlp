{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b43831e-ada3-469b-bc2f-e8564651fcb1",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing For token classification\n",
    "## for Encouragement Approach (Discourage avoidance) only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaeff7d-4e1f-4a31-882d-24dea599f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb146cc9-c05c-4ba2-87ca-03916055e0b8",
   "metadata": {},
   "source": [
    "# 1. All data without any filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b339f39-ae57-48d1-8693-c63ff5a25ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '/home/lossioventuraj2/Code/07-Transcripts/words_w_exp_fine_V1/'\n",
    "subsets = ['2_training', '3_validation', '4_testing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e20a0-b57b-4fc8-a0cf-8b76035be601",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataframes = []\n",
    "csv_per_set = {}\n",
    "\n",
    "list_length_tokens = []\n",
    "\n",
    "for subset_ in subsets:\n",
    "    name_dataset = subset_[2:]\n",
    "    dataframes = {}\n",
    "    print(name_dataset)\n",
    "    list_files = os.listdir(path_dataset + subset_)\n",
    "    print(f\"\\tNumber of files: {len(list_files)}\")\n",
    "    for file_ in list_files:\n",
    "        df_aux = pd.read_csv(path_dataset + subset_ + \"/\" + file_)\n",
    "        dataframes[file_[:-4]] = df_aux\n",
    "        list_length_tokens.append(df_aux.shape[0])\n",
    "        list_dataframes.append(df_aux)\n",
    "        #print(df_aux.shape)\n",
    "\n",
    "    csv_per_set[name_dataset] = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cea51c-8562-4442-a763-0804fbb9104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'id': Value('string'),\n",
    "    'tokens': Value('string'),\n",
    "    'labels': ClassLabel(names=[\"NoDA\",\"DA\"])  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a554cf-338e-48c0-88bd-3f5369442c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(csv_per_set['training']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180461c4-82b2-4acc-b49a-5170abf420cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_Datasets = {}\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "for set_, dict_of_dataframes in csv_per_set.items():\n",
    "    print(set_)\n",
    "    print(len(dict_of_dataframes))\n",
    "\n",
    "    dataset_list = []\n",
    "    for name, df in dict_of_dataframes.items():\n",
    "        tokens = df['WORD'].tolist()\n",
    "        labels = df['DiscourageAvoidance'].tolist()\n",
    "    \n",
    "        #print(len(tokens))\n",
    "        for i in range(0, len(tokens), MAX_LENGTH):\n",
    "            #print(i)\n",
    "            dict_aux_ = {\n",
    "                'id': str(name) + f\"_part{i // MAX_LENGTH + 1}\",  # Convert ID to string\n",
    "                'tokens': [str(token) for token in tokens[i:i + MAX_LENGTH]],  # Make sure string type\n",
    "                'ner_tags': [int(label) for label in labels[i:i + MAX_LENGTH]]   # Make sure string type\n",
    "            }\n",
    "            dataset_list.append(dict_aux_)\n",
    "\n",
    "    dataset_aux = Dataset.from_list(dataset_list)\n",
    "    list_for_Datasets[set_] = dataset_aux\n",
    "\n",
    "dataset_dict = DatasetDict(list_for_Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e954d1-8595-48e6-aaee-61e1c6c49abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset_dict))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63f5fc-10fc-41e9-bcc6-7bc4bc1b6603",
   "metadata": {},
   "source": [
    "# Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad437a9-41b8-4977-a7c6-4dbd0dbace29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict.save_to_disk(\"discourage_avoidance_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b1541-dced-4c5a-95f8-840674a8201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = dataset_dict[\"training\"]\n",
    "print(type(training_dataset))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f7648-a5d9-49f2-a5fc-0e99cccf28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298c37f-2510-4d7f-9c2e-d9be5a8a8e21",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82682f6-6e3d-491c-8029-44b6e09ece10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Load the DatasetDict from the saved directory\n",
    "loaded_dataset_dict = DatasetDict.load_from_disk(\"discourage_avoidance_all\")\n",
    "\n",
    "# Now `loaded_dataset_dict` is the same as the original dataset_dict\n",
    "print(loaded_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13b5f6-1329-4d7a-81bf-435e75900b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5974ae26-2acc-4cec-abe9-14ab53d89da7",
   "metadata": {},
   "source": [
    "# 2. Changing sequence lower than 10 to 0\n",
    "# Since it might add noise instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3cbee-fe8e-4288-93bb-dc71c06d67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03374d4-1ffe-4c0d-a213-3171343106a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '/home/lossioventuraj2/Code/07-Transcripts/words_w_exp_fine_V1/'\n",
    "subsets = ['2_training', '3_validation', '4_testing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb67bf-aa45-436f-b97c-6a11ae235ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_short_sequences(df):\n",
    "    df = df.copy()  # Copy to avoid SettingWithCopyWarning\n",
    "    in_sequence = False\n",
    "    start_idx = None\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, \"DiscourageAvoidance\"] == 1:\n",
    "            if not in_sequence:\n",
    "                in_sequence = True\n",
    "                start_idx = i\n",
    "        else:\n",
    "            if in_sequence:\n",
    "                in_sequence = False\n",
    "                end_idx = i\n",
    "                length = end_idx - start_idx\n",
    "\n",
    "                if length < THRESHOLD:\n",
    "                    df.loc[start_idx:end_idx, \"DiscourageAvoidance\"] = 0  # Make sure modification\n",
    "    \n",
    "    if in_sequence and (len(df) - start_idx) < THRESHOLD:\n",
    "        df.loc[start_idx:, \"DiscourageAvoidance\"] = 0\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218dfec-079e-496a-89ad-ba51de99294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataframes = []\n",
    "csv_per_set = {}\n",
    "\n",
    "list_length_tokens = []\n",
    "\n",
    "for subset_ in subsets:\n",
    "    name_dataset = subset_[2:]\n",
    "    dataframes = {}\n",
    "    print(name_dataset)\n",
    "    list_files = os.listdir(path_dataset + subset_)\n",
    "    print(f\"\\tNumber of files: {len(list_files)}\")\n",
    "    for file_ in list_files:\n",
    "        df_aux = pd.read_csv(path_dataset + subset_ + \"/\" + file_)\n",
    "        dataframes[file_[:-4]] = df_aux\n",
    "        list_length_tokens.append(df_aux.shape[0])\n",
    "        list_dataframes.append(df_aux)\n",
    "\n",
    "    csv_per_set[name_dataset] = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1c25f-ed46-490f-ad4e-0e487aa8af9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_for_Datasets = {}\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "for set_, dict_of_dataframes in csv_per_set.items():\n",
    "    print(set_)\n",
    "    print(len(dict_of_dataframes))\n",
    "\n",
    "    dataset_list = []\n",
    "    for name, df in dict_of_dataframes.items():\n",
    "\n",
    "        print(\"Before processing:\\n\")\n",
    "        print(df[\"DiscourageAvoidance\"].value_counts())\n",
    "        df = modify_short_sequences(df)\n",
    "        print(\"\\nAfter processing:\\n\")\n",
    "        print(df[\"DiscourageAvoidance\"].value_counts())\n",
    "        print(\"\\n####################################\\n\")\n",
    "        \n",
    "        tokens = df['WORD'].tolist()\n",
    "        labels = df['DiscourageAvoidance'].tolist()\n",
    "    \n",
    "        #print(len(tokens))\n",
    "        for i in range(0, len(tokens), MAX_LENGTH):\n",
    "            #print(i)\n",
    "            dict_aux_ = {\n",
    "                'id': str(name) + f\"_part{i // MAX_LENGTH + 1}\",  # Convert ID to string\n",
    "                'tokens': [str(token) for token in tokens[i:i + MAX_LENGTH]],  # Make Sure string type\n",
    "                'ner_tags': [int(label) for label in labels[i:i + MAX_LENGTH]]   # Make Sure string type\n",
    "            }\n",
    "            dataset_list.append(dict_aux_)\n",
    "\n",
    "    dataset_aux = Dataset.from_list(dataset_list)\n",
    "    list_for_Datasets[set_] = dataset_aux\n",
    "\n",
    "dataset_dict_filtered = DatasetDict(list_for_Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e973b-9a9c-494f-bbfd-917f0be59e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset_dict_filtered))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb4aa2-4b5e-4718-baaa-c4c5e648ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict_filtered.save_to_disk(\"discourage_avoidance_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67794c78-0198-4197-a361-d16ae7f917a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c540d04-5bbc-439a-8b58-f36d2e5c7ece",
   "metadata": {},
   "source": [
    "# 3. Merge close sequences\n",
    "## If the gap (number of 0s) between a sequence end and the next sequence start \n",
    "## is less than 10, convert those 0s into 1s. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a062d20f-548f-4e50-956a-116c302b44c5",
   "metadata": {},
   "source": [
    "ID_Token  DiscourageAvoidance\n",
    "8423      1\n",
    "8424      1\n",
    "8425      1\n",
    "8426      1\n",
    "8427      1\n",
    "8428      0  <- Gap of 6 (less than 10)\n",
    "8429      0\n",
    "8430      0\n",
    "8431      0\n",
    "8432      0\n",
    "8433      1\n",
    "8434      1\n",
    "8435      1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26feca8c-ce9e-4bc5-b7a8-743581038cb9",
   "metadata": {},
   "source": [
    "ID_Token  DiscourageAvoidance\n",
    "8423      1\n",
    "8424      1\n",
    "8425      1\n",
    "8426      1\n",
    "8427      1\n",
    "8428      1  <- Gap filled\n",
    "8429      1\n",
    "8430      1\n",
    "8431      1\n",
    "8432      1\n",
    "8433      1\n",
    "8434      1\n",
    "8435      1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b51f1-9ec8-45b4-b833-12ea80b392fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89adc340-fd59-4bc5-b516-6b59025c8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca9453-7f0c-49f0-997f-6ea2da0ad857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_close_sequences(df):\n",
    "    df = df.copy()\n",
    "    in_sequence = False\n",
    "    sequences = []  # Store (start, end) of sequences\n",
    "\n",
    "    # Identify sequences of 1s\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, \"DiscourageAvoidance\"] == 1:\n",
    "            if not in_sequence:\n",
    "                start_idx = i\n",
    "                in_sequence = True\n",
    "        else:\n",
    "            if in_sequence:\n",
    "                end_idx = i - 1  # Last 1 in the sequence\n",
    "                sequences.append((start_idx, end_idx))\n",
    "                in_sequence = False\n",
    "\n",
    "    # Handle case if the sequence extends to the last row\n",
    "    if in_sequence:\n",
    "        sequences.append((start_idx, len(df) - 1))\n",
    "\n",
    "    # If no sequences were found, return the DataFrame unchanged\n",
    "    if not sequences:\n",
    "        return df\n",
    "\n",
    "    # Merge sequences that are close together\n",
    "    merged_sequences = []\n",
    "    prev_start, prev_end = sequences[0]\n",
    "\n",
    "    for i in range(1, len(sequences)):\n",
    "        curr_start, curr_end = sequences[i]\n",
    "        \n",
    "        if (curr_start - prev_end) < 10:  # If the gap is less than 10\n",
    "            prev_end = curr_end  # Merge the sequence\n",
    "        else:\n",
    "            merged_sequences.append((prev_start, prev_end))\n",
    "            prev_start, prev_end = curr_start, curr_end\n",
    "    \n",
    "    merged_sequences.append((prev_start, prev_end))  # Add the last sequence\n",
    "\n",
    "    # Preserve existing 1s and update only necessary 0s\n",
    "    for start, end in merged_sequences:\n",
    "        df.loc[start:end, \"DiscourageAvoidance\"] = 1  # Make sure all merged areas are 1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b4296-7b68-42f2-a427-05bdafd8492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataframes = []\n",
    "csv_per_set = {}\n",
    "\n",
    "list_length_tokens = []\n",
    "\n",
    "for subset_ in subsets:\n",
    "    name_dataset = subset_[2:]\n",
    "    dataframes = {}\n",
    "    print(name_dataset)\n",
    "    list_files = os.listdir(path_dataset + subset_)\n",
    "    print(f\"\\tNumber of files: {len(list_files)}\")\n",
    "    for file_ in list_files:\n",
    "        df_aux = pd.read_csv(path_dataset + subset_ + \"/\" + file_)\n",
    "        dataframes[file_[:-4]] = df_aux\n",
    "        list_length_tokens.append(df_aux.shape[0])\n",
    "        list_dataframes.append(df_aux)\n",
    "        #print(df_aux.shape)\n",
    "\n",
    "    csv_per_set[name_dataset] = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79817d-c0fa-4098-9d87-89990da3b1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_for_Datasets = {}\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "for set_, dict_of_dataframes in csv_per_set.items():\n",
    "    print(set_)\n",
    "    print(len(dict_of_dataframes))\n",
    "\n",
    "    dataset_list = []\n",
    "    for name, df in dict_of_dataframes.items():\n",
    "        df = df.copy()\n",
    "        print(\"Before processing:\")\n",
    "        print(df[\"DiscourageAvoidance\"].value_counts())\n",
    "        df = merge_close_sequences(df)\n",
    "        print(\"After processing:\")\n",
    "        print(df[\"DiscourageAvoidance\"].value_counts())\n",
    "        \n",
    "        tokens = df['WORD'].tolist()\n",
    "        labels = df['DiscourageAvoidance'].tolist()\n",
    "    \n",
    "        #print(len(tokens))\n",
    "        for i in range(0, len(tokens), MAX_LENGTH):\n",
    "            #print(i)\n",
    "            dict_aux_ = {\n",
    "                'id': str(name) + f\"_part{i // MAX_LENGTH + 1}\",  # Convert ID to string\n",
    "                'tokens': [str(token) for token in tokens[i:i + MAX_LENGTH]],  # Make sure string type\n",
    "                'ner_tags': [int(label) for label in labels[i:i + MAX_LENGTH]]   # Make sure string type\n",
    "            }\n",
    "            dataset_list.append(dict_aux_)\n",
    "\n",
    "    dataset_aux = Dataset.from_list(dataset_list)\n",
    "    list_for_Datasets[set_] = dataset_aux\n",
    "\n",
    "dataset_dict_filled = DatasetDict(list_for_Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105eaed-a917-4a85-95d6-8bb7a27c35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ID\": str(name_file),\n",
    "\"Exposure_Number\": str(number_exposure),\n",
    "\"ID_ExNum\": str(name), \n",
    "\"Sub_ID\": int(i+1),\n",
    "\"Indexes\": [int(indx_) for indx_ in range(i,i + window_size)],\n",
    "\"tokens\": [str(token) for token in window],  #window,\n",
    "\"ner_tags\": [str(cat) for cat in window_label] #window_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b53d13-b2b5-4ca1-9b82-b9d941f2e161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_for_Datasets = {}\n",
    "MAX_LENGTH = 250\n",
    "\n",
    "for set_, dict_of_dataframes in csv_per_set.items():\n",
    "    print(set_)\n",
    "    print(len(dict_of_dataframes))\n",
    "\n",
    "    dataset_list = []\n",
    "    for name, df in dict_of_dataframes.items():\n",
    "        \n",
    "        name_file = name[:-17]\n",
    "        number_exposure = name[-4:]\n",
    "        \n",
    "        df = df.copy()\n",
    "        print(\"Before processing:\")\n",
    "        print(df[\"DiscourageAvoidance\"].value_counts())\n",
    "        df = merge_close_sequences(df)\n",
    "        print(\"After processing:\")\n",
    "        print(df[\"DiscourageAvoidance\"].value_counts())\n",
    "        \n",
    "        tokens = df['WORD'].tolist()\n",
    "        labels = df['DiscourageAvoidance'].tolist()\n",
    "    \n",
    "        #print(len(tokens))\n",
    "        for i in range(0, len(tokens), MAX_LENGTH):\n",
    "            #print(i)\n",
    "            dict_aux_ = {\n",
    "                \"ID\": str(name_file),\n",
    "                \"Exposure_Number\": str(number_exposure),\n",
    "                \"ID_ExNum\": str(name), \n",
    "                \"Sub_ID\": int(i // MAX_LENGTH + 1),\n",
    "                'tokens': [str(token) for token in tokens[i:i + MAX_LENGTH]],  # Make sure string type\n",
    "                'ner_tags': [int(label) for label in labels[i:i + MAX_LENGTH]]   # Make sure string type\n",
    "            }\n",
    "            dataset_list.append(dict_aux_)\n",
    "\n",
    "    dataset_aux = Dataset.from_list(dataset_list)\n",
    "    list_for_Datasets[set_] = dataset_aux\n",
    "\n",
    "dataset_dict_filled = DatasetDict(list_for_Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2aef3-0fe2-494c-baf1-b9c961779f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict_filled.save_to_disk(\"datasets/no_slide_windows/window_250/discourage_avoidance_merged_alt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1e40f-7b5c-4186-97d2-e312ac2cf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5a2e3-eeb2-4cb1-b340-8cd27d9cdc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5c534-d6c7-43d3-ba67-49a0e81fee68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7fd4c-c2f2-4b3e-b9ed-7a65edd0f660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418a301-4ca3-4f42-940b-9297a0b5ec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
